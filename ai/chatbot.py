import os
from dotenv import load_dotenv
from ai.qdrantdb import get_embeddings, get_vector_config
from langchain_qdrant import QdrantVectorStore
from langchain_mistralai import ChatMistralAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter
from ai.tools.mcp_bridge import call_mcp_jira_ticket
from langchain_core.tools import tool
from langchain_core.tools import tool


from pathlib import Path

env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

store = {}

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

@tool
def create_atlassian_ticket(category: str, summary: str, description: str, priority: str):
    """
    Cr√©e un ticket Jira via MCP si le d√©pannage assist√© par l'IA √©choue.
    - category: Doit √™tre 'installation', 'maintenance', 'depannage' ou 'peripherique'.
    - summary: Titre court du probl√®me (ex: Fuite Vanne V-12).
    - description: R√©sum√© technique complet et historique des tests effectu√©s.
    - priority: Niveau d'urgence (High, Medium, Low).
    """
    
    # 1. Mapping pour l'assignation automatique (Assignee)
    groups = {
        "installation": "Groupe Installation",
        "maintenance": "Groupe Maintenance",
        "depannage": "Groupe D√©pannage",
        "peripherique": "Groupe P√©riph√©riques"
    }
    
    # On r√©cup√®re le nom du groupe correspondant √† la cat√©gorie choisie par l'IA
    assignee_group = groups.get(category.lower(), "Support G√©n√©ral")

    # 2. Appel du pont (bridge) vers le serveur Node.js MCP
    # Cette fonction va envoyer le JSON-RPC vers l'entr√©e standard (stdin)
    result = call_mcp_jira_ticket(summary, description, priority, assignee_group)
    
    return f"R√©sultat : {result}"



def get_chatbot_chain():
    #Connexion √† la base de donn√©es Qdrant
    db_params = get_vector_config()
    
    vectorstore = QdrantVectorStore.from_existing_collection(
        embedding=get_embeddings(),
        collection_name=db_params["collection_name"],
        url=db_params["url"],
        api_key=db_params["api_key"],
    )

    #chercher les 3 meilleurs morceaux
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )

    #Configuration du mod√®le LLM
    llm = ChatMistralAI(
        model="mistral-large-latest", 
        api_key=os.getenv("MISTRAL_API_KEY"),
        temperature=0.2
    )
    
    llm_with_tools = llm.bind_tools([create_atlassian_ticket])
    
    #Instructions
    system_prompt = (
        """R√¥le : Tu es un assistant technique expert sp√©cialis√© exclusivement dans l'installation, la maintenance et le d√©pannage des √©lectrovannes et des vannes de zone.
        - R√©ponds de fa√ßon pr√©cise et courte, tu dois resumer les informations techniques pertinentes.

        Domaine d'expertise :
        Ton p√©rim√®tre d'intervention est strictement limit√© aux sujets suivants :
        1. Installation : V√©rification des propri√©t√©s (tension/fr√©quence de bobine, pression), sens de montage, c√¢blage et mise en service.
        2. Maintenance : Nettoyage des composants internes (plongeur, ressort, joints), inspection de la corrosion et remplacement de pi√®ces.
        3. D√©pannage : Diagnostic de pannes (bruit, surchauffe de bobine, fuites de membrane, probl√®mes de pression).
        4. P√©riph√©riques : R√©gulateurs de pression d'air et actionneurs pneumatiques.

        Instructions de refus :
        - Si la premiere question de la conversation n'est pas clair ou manque de contexte technique, demande des pr√©cisions avant de r√©pondre.
        - Si c'est pas la premiere question, utilise le contexte de la conversation pour clarifier.
        - Si la question est hors sujet (ex: cuisine, conseils juridiques, plomberie g√©n√©rale non li√©e aux vannes), d√©cline poliment la demande. 
        - Exemple de refus : "Je suis d√©sol√©, mais mon expertise est limit√©e aux √©lectrovannes. Je ne peux pas r√©pondre √† votre question sur [le sujet concern√©]."
        - Si tu ne connais pas la r√©ponse, dis simplement que tu ne sais pas.
        - Rappelle toujours de couper l'alimentation et de d√©pressuriser avant manipulation.

        Instructions de Ticketing :
        - Si l'utilisateur exprime que les solutions propos√©es n'ont pas fonctionn√©, ou si le probl√®me persiste apr√®s manipulation, tu DOIS proposer de cr√©er un ticket.
        - Une fois que l'utilisateur est d'accord ou si la situation est critique, utilise l'outil 'create_atlassian_ticket'.
        - Pour le champ 'category', analyse le probl√®me et choisis parmi : 'installation', 'maintenance', 'depannage', ou 'peripherique'.
        - Pour le champ 'summary', fournis un titre court et descriptif du probl√®me.
        - Pour le champ 'description', fournis un r√©sum√© technique complet incluant l'historique des tests effectu√©s.
        - Pour le champ 'priority', √©value l'urgence comme 'High', 'Medium', ou 'Low' en fonction de l'impact sur les op√©rations, utilise la logique suivante , tu dois toi meme savoir √©valuer le niveau de priorit√© :
            * 'High' : Fuite majeure, vanne bloqu√©e sur un circuit critique, risque de surchauffe √©lectrique ou de court-circuit.
            * 'Medium' : Bruit anormal persistant, vanne lente √† r√©agir, ou maintenance pr√©ventive n√©cessaire sur un √©quipement actif.
            * 'Low' : L√©g√®re trace de corrosion sans impact imm√©diat, demande de v√©rification de c√¢blage non urgente, ou demande d'information technique suite √† une installation.
        
        Extraits techniques √† utiliser :
        {context}"""
    )

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ])
    
    #"traducteur" qui transforme les r√©sultats de la base de donn√©es en un texte lisible pour l'IA.
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)


    # 5. Assemblage de la cha√Æne
    chain = (
        {
            "context": itemgetter("input") | retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)),
            "input": itemgetter("input"),
            "chat_history": itemgetter("chat_history") 
        }
        | prompt_template
        | llm_with_tools
        
        # On s'arr√™te ici ! On ne met pas | StrOutputParser() car main g√®re le streaming , juste pour le test on le met
        # Supprime StrOutputParser ici si on veux g√©rer les appels d'outils proprement, 
        # ou garde-le si on ne veux streamer que le texte.
    )
         # w9ila anmsh had l commentaire pour gemini ajouter | StrOutputParser()
    
    return RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
    )
        
    
if __name__ == "__main__":
    bot = get_chatbot_chain()
    config = {"configurable": {"session_id": "amine_conv_1"}}
    
    """
        # --- Question 1 ---
    print("\nü§ñ Chatbot: ", end="")
    for chunk in bot.stream({"input": "Causes fuite √©lectrovanne ?"}, config=config):
        print(chunk, end="", flush=True)
    print()
    """
    # --- Question 2 ---
    print("\nü§ñ Chatbot: ", end="")
    for chunk in bot.stream({"input": "saluut"}, config=config):
        print(chunk, end="", flush=True)
    
    """
    print("\nü§ñ Chatbot: ", end="")
    for chunk in bot.stream({"input": "√ßa ne fonctionne pas pour moi ! "}, config=config):
        print(chunk, end="", flush=True)
    
    print("\nü§ñ Chatbot: ", end="")
    for chunk in bot.stream({"input": "oui! "}, config=config):
        print(chunk, end="", flush=True)
    """
    
    """
    print("\nü§ñ Chatbot: ", end="")
    for chunk in bot.stream({"input": "Quel est le meilleur club de football au monde ?"}, config=config):
        print(chunk, end="", flush=True)
       
    """
    
     
    
    print()